{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bc9df6",
   "metadata": {},
   "source": [
    "# Deep learning for dynamic network analysis (DLDNA) - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc303ce",
   "metadata": {},
   "source": [
    "Dolphins: R. ARNAUD M. DELPLANQUE A. KARILA-COHEN A. RAMPOLDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913557e",
   "metadata": {},
   "source": [
    "Comprehensive soil classification dataset: https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10532874",
   "metadata": {},
   "source": [
    "CNN puis GAN puis CyGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd56d4",
   "metadata": {},
   "source": [
    "Binary classification : Binary CrossEntropy Loss $ \\mathcal{L}_{\\text{BCE}}(y,\\hat y)\n",
    "= - \\left[ y \\log(\\hat y) + (1-y)\\log(1-\\hat y) \\right]\n",
    " $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73b84e",
   "metadata": {},
   "source": [
    "Rappel: le learning rate $ \\alpha $ est le pas de mise à jour lors de la descente de gradient. Formule de la descente de gradient $ L(\\theta_{n+1})= L(\\theta_{n})-\\alpha \\nabla  L(\\theta_{n}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9363973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings to keep notebook clean\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# General\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TRAIN_RATIO  = 0.7\n",
    "VAL_RATIO    = 0.1\n",
    "TEST_RATIO   = 0.2\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Use parameters for seed and device\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"Params loaded. Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40d16d",
   "metadata": {},
   "source": [
    "Penser à convertir les X et y en tenseur torch avant de procéder au datasplit via ``sklearn.model_selection.train_test_split`` <br>\n",
    "```X = torch.FloatTensor(X) ``` et ```y = torch.FloatTensor(y) ``` <br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99004037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # First split: separate out test set (30% of total)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=SEED  # 70% train, 30% temp\n",
    "# )\n",
    "# # Second split: split temp into validation (10%) and test (20%)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=2/3, random_state=SEED  # 10% val, 20% test\n",
    "# )\n",
    "# print(f\"Training samples: {len(X_train)}\")  # Show split sizes\n",
    "# print(f\"Validation samples: {len(X_val)}\")\n",
    "# print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23bc75",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "Standardizer les data pour RGB et passer de 0 à 255 à 0 à 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc50c5",
   "metadata": {},
   "source": [
    "**My first CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cacc4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28 → 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 28x28 → 14x14\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 14x14 → 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                              # 14x14 → 7x7\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e514c3",
   "metadata": {},
   "source": [
    "**Definition of the MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = SimpleCNN().to(device)  # Create model and move to device\n",
    "regression_loss_fn = nn.BCELoss()  # Binary Cross Entropy loss\n",
    "regression_optimizer = Adam(regression_model.parameters(), lr=0.01)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093ae72",
   "metadata": {},
   "source": [
    "**Grid search (hyperparameters optimisation for Adam GD): hidden size and learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b81d",
   "metadata": {},
   "source": [
    "If the learning rate is too big, the gradient descent cannot be stable. In a contrary if it is too small, the learning can be slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a22c",
   "metadata": {},
   "source": [
    "**Training with validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=100):\n",
    "    \"\"\"Train the model with validation monitoring.\"\"\"\n",
    "    train_losses = []  # Track training losses\n",
    "    val_losses = []  # Track validation losses\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()  # Set to training mode\n",
    "        train_loss = 0.0  # Reset epoch loss\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device),  yb.to(device)  # Move to device\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            preds = model(xb)  # Forward pass\n",
    "            loss = loss_fn(preds, yb)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            train_loss += loss.item()  # Accumulate loss\n",
    "        train_loss /= len(train_loader)  # Average loss\n",
    "        train_losses.append(train_loss)  # Store loss\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        val_loss = 0.0  # Reset validation loss\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device),  yb.to(device)  # Move to device\n",
    "                preds = model(xb)  # Forward pass\n",
    "                val_loss += loss_fn(preds, yb).item()  # Compute loss\n",
    "            val_loss /= len(val_loader)  # Average loss\n",
    "            val_losses.append(val_loss)  # Store loss\n",
    "            \n",
    "        if epoch % 10 == 0 or epoch == 1:  # Print every 10 epochs\n",
    "            print(f\"Epoch {epoch:03d} | Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9649c6f",
   "metadata": {},
   "source": [
    "**Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67953dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn, device):\n",
    "    \"\"\"Perform final testing on the model using the held-out test set.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    test_loss = 0.0  # Initialize test loss\n",
    "    all_preds = []  # Store all predictions\n",
    "    all_actuals = []  # Store all actual values\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)  # Move batch to device\n",
    "            yb = yb.to(device)  # Move labels to device\n",
    "            preds = model(xb)  # Forward pass\n",
    "            test_loss += loss_fn(preds, yb).item()  # Accumulate loss\n",
    "            all_preds.append(preds)  # Store predictions\n",
    "            all_actuals.append(yb)  # Store actuals\n",
    "            \n",
    "    test_loss /= len(test_loader)  # Average loss\n",
    "    all_preds = torch.cat(all_preds)  # Concatenate all predictions\n",
    "    all_actuals = torch.cat(all_actuals)  # Concatenate all actuals\n",
    "    \n",
    "    return test_loss, all_preds, all_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f1d59",
   "metadata": {},
   "source": [
    "**Test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a4c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 5e-4] \n",
    "hidden_sizes_options = [(32,16),(64,32), (128,64)]\n",
    "\n",
    "def grid_search_hyperparameters(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    learning_rates,\n",
    "    hidden_sizes_options,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    base_model=None,\n",
    "    model_fn=None,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid search over learning rates and hidden layer sizes.\n",
    "    - model_fn: callable taking hidden_sizes (e.g., (h1, h2)) and returning an nn.Module (on CPU).\n",
    "    - hidden_sizes_options: list of tuples like [(64,32), (128,64), ...]\n",
    "    Saves the globally best model (by validation accuracy) to 'save_path'.\n",
    "    Returns: (results_list, best_cfg_dict, best_model_loaded)\n",
    "    \"\"\"\n",
    "    assert model_fn is not None, \"Provide model_fn(hidden_sizes) -> nn.Module\"\n",
    "\n",
    "    results = []  # Store all results\n",
    "    best_val_acc = -1.0  # Track best validation accuracy\n",
    "    best_cfg = None  # Track best configuration\n",
    "    best_state = None  # Track best model state\n",
    "\n",
    "    for lr in learning_rates:  # Loop over learning rates\n",
    "        for hidden_sizes in hidden_sizes_options:  # Loop over architectures\n",
    "            print(f\"Testing: lr={lr}, hidden_sizes={hidden_sizes}\")\n",
    "\n",
    "            model = model_fn(hidden_sizes).to(device)  # Create model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Create optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()  # Define loss function\n",
    "\n",
    "            # Train model\n",
    "            _, _, _, train_accuracies, val_accuracies = train_with_validation(\n",
    "                model=model,  # Model to train\n",
    "                train_loader=train_loader,  # Training data\n",
    "                val_loader=val_loader,  # Validation data\n",
    "                optimizer=optimizer,  # Optimizer\n",
    "                loss_fn=loss_fn,  # Loss function\n",
    "                device=device,  # Device\n",
    "                epochs=epochs,  # Number of epochs\n",
    "                task_type='classification'  # Task type\n",
    "            )\n",
    "\n",
    "            cur_best_val = max(val_accuracies)  # Get best validation accuracy\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'lr': lr,  # Learning rate\n",
    "                'hidden_sizes': hidden_sizes,  # Architecture\n",
    "                'best_val_acc': cur_best_val,  # Best validation accuracy\n",
    "                'final_train_acc': train_accuracies[-1],  # Final training accuracy\n",
    "                'final_val_acc': val_accuracies[-1]  # Final validation accuracy\n",
    "            })\n",
    "\n",
    "            print(f\"Best validation accuracy: {cur_best_val:.2f}%\")\n",
    "\n",
    "            # Update best model if this is better\n",
    "            if cur_best_val > best_val_acc:\n",
    "                best_val_acc = cur_best_val  # Update best accuracy\n",
    "                best_cfg = {'lr': lr, 'hidden_sizes': hidden_sizes}  # Update best config\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}  # Save state to CPU\n",
    "                if save_path is not None:\n",
    "                    torch.save(best_state, save_path)  # Save to disk\n",
    "                    print(f\"Saved new best model to: {save_path}\")\n",
    "\n",
    "            del model  # Free memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "\n",
    "    # Rebuild best model\n",
    "    best_model = None\n",
    "    if best_state is not None:\n",
    "        best_model = model_fn(best_cfg['hidden_sizes']).to(device)  # Create model\n",
    "        best_model.load_state_dict(best_state)  # Load best weights\n",
    "\n",
    "\n",
    "    return results, best_cfg, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469136c",
   "metadata": {},
   "source": [
    "**Train validate and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the model with validation monitoring\n",
    "model, train_losses, val_losses = train_with_validation(\n",
    "    model=regression_model,  # Model to train\n",
    "    train_loader=train_loader,  # Training data\n",
    "    val_loader=val_loader,  # Validation data\n",
    "    optimizer=regression_optimizer,  # Optimizer\n",
    "    loss_fn=regression_loss_fn,  # Loss function\n",
    "    device=device,  # Device (CPU/GPU)\n",
    "    epochs=100  # Number of epochs\n",
    ")\n",
    "\n",
    "# Step 2: Perform final testing on held-out test set\n",
    "test_loss, test_preds, test_actuals = test_model(\n",
    "    model=model,  # Trained model\n",
    "    test_loader=test_loader,  # Test data\n",
    "    loss_fn=regression_loss_fn,  # Loss function\n",
    "    device=device  # Device\n",
    ")\n",
    "\n",
    "# Print evaluation metrics\n",
    "avg_train_loss = sum(train_losses) / len(train_losses)  # Calculate average training loss\n",
    "avg_val_loss = sum(val_losses) / len(val_losses)  # Calculate average validation loss\n",
    "print(f\"Average Training MSE: {avg_train_loss:.4f} | \"\n",
    "      f\"Average Validation MSE: {avg_val_loss:.4f} | \"\n",
    "      f\"Test MSE: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
