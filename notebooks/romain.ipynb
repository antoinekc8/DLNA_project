{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bc9df6",
   "metadata": {},
   "source": [
    "# Deep learning for dynamic network analysis (DLDNA) - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc303ce",
   "metadata": {},
   "source": [
    "Dolphins: R. ARNAUD M. DELPLANQUE A. KARILA-COHEN A. RAMPOLDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913557e",
   "metadata": {},
   "source": [
    "Comprehensive soil classification dataset: https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10532874",
   "metadata": {},
   "source": [
    "CNN puis GAN puis CyGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd56d4",
   "metadata": {},
   "source": [
    "Binary classification : Binary CrossEntropy Loss $ \\mathcal{L}_{\\text{BCE}}(y,\\hat y)\n",
    "= - \\left[ y \\log(\\hat y) + (1-y)\\log(1-\\hat y) \\right]\n",
    " $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73b84e",
   "metadata": {},
   "source": [
    "Rappel: le learning rate $ \\alpha $ est le pas de mise à jour lors de la descente de gradient. Formule de la descente de gradient $ L(\\theta_{n+1})= L(\\theta_{n})-\\alpha \\nabla  L(\\theta_{n}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9363973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings to keep notebook clean\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# General\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TRAIN_RATIO  = 0.7\n",
    "VAL_RATIO    = 0.1\n",
    "TEST_RATIO   = 0.2\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Use parameters for seed and device\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"Params loaded. Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40d16d",
   "metadata": {},
   "source": [
    "Penser à convertir les X et y en tenseur torch avant de procéder au datasplit via ``sklearn.model_selection.train_test_split`` <br>\n",
    "```X = torch.FloatTensor(X) ``` et ```y = torch.FloatTensor(y) ``` <br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99004037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # First split: separate out test set (30% of total)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=SEED  # 70% train, 30% temp\n",
    "# )\n",
    "# # Second split: split temp into validation (10%) and test (20%)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=2/3, random_state=SEED  # 10% val, 20% test\n",
    "# )\n",
    "# print(f\"Training samples: {len(X_train)}\")  # Show split sizes\n",
    "# print(f\"Validation samples: {len(X_val)}\")\n",
    "# print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23bc75",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "Standardizer les data pour RGB et passer de 0 à 255 à 0 à 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e514c3",
   "metadata": {},
   "source": [
    "**Definition of the MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = MLPRegressor().to(device)  # Create model and move to device\n",
    "regression_loss_fn = nn.BCELoss()  # Binary Cross Entropy loss\n",
    "regression_optimizer = Adam(regression_model.parameters(), lr=0.01)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093ae72",
   "metadata": {},
   "source": [
    "**Grid search (hyperparameters optimisation for Adam GD): hidden size and learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b81d",
   "metadata": {},
   "source": [
    "If the learning rate is too big, the gradient descent cannot be stable. In a contrary if it is too small, the learning can be slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9a4c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 5e-4] \n",
    "hidden_sizes_options = [(32,16),(64,32), (128,64)]\n",
    "\n",
    "def grid_search_hyperparameters(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    learning_rates,\n",
    "    hidden_sizes_options,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    base_model=None,\n",
    "    model_fn=None,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid search over learning rates and hidden layer sizes.\n",
    "    - model_fn: callable taking hidden_sizes (e.g., (h1, h2)) and returning an nn.Module (on CPU).\n",
    "    - hidden_sizes_options: list of tuples like [(64,32), (128,64), ...]\n",
    "    Saves the globally best model (by validation accuracy) to 'save_path'.\n",
    "    Returns: (results_list, best_cfg_dict, best_model_loaded)\n",
    "    \"\"\"\n",
    "    assert model_fn is not None, \"Provide model_fn(hidden_sizes) -> nn.Module\"\n",
    "\n",
    "    results = []  # Store all results\n",
    "    best_val_acc = -1.0  # Track best validation accuracy\n",
    "    best_cfg = None  # Track best configuration\n",
    "    best_state = None  # Track best model state\n",
    "\n",
    "    for lr in learning_rates:  # Loop over learning rates\n",
    "        for hidden_sizes in hidden_sizes_options:  # Loop over architectures\n",
    "            print(f\"Testing: lr={lr}, hidden_sizes={hidden_sizes}\")\n",
    "\n",
    "            model = model_fn(hidden_sizes).to(device)  # Create model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Create optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()  # Define loss function\n",
    "\n",
    "            # Train model\n",
    "            _, _, _, train_accuracies, val_accuracies = train_with_validation(\n",
    "                model=model,  # Model to train\n",
    "                train_loader=train_loader,  # Training data\n",
    "                val_loader=val_loader,  # Validation data\n",
    "                optimizer=optimizer,  # Optimizer\n",
    "                loss_fn=loss_fn,  # Loss function\n",
    "                device=device,  # Device\n",
    "                epochs=epochs,  # Number of epochs\n",
    "                task_type='classification'  # Task type\n",
    "            )\n",
    "\n",
    "            cur_best_val = max(val_accuracies)  # Get best validation accuracy\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'lr': lr,  # Learning rate\n",
    "                'hidden_sizes': hidden_sizes,  # Architecture\n",
    "                'best_val_acc': cur_best_val,  # Best validation accuracy\n",
    "                'final_train_acc': train_accuracies[-1],  # Final training accuracy\n",
    "                'final_val_acc': val_accuracies[-1]  # Final validation accuracy\n",
    "            })\n",
    "\n",
    "            print(f\"Best validation accuracy: {cur_best_val:.2f}%\")\n",
    "\n",
    "            # Update best model if this is better\n",
    "            if cur_best_val > best_val_acc:\n",
    "                best_val_acc = cur_best_val  # Update best accuracy\n",
    "                best_cfg = {'lr': lr, 'hidden_sizes': hidden_sizes}  # Update best config\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}  # Save state to CPU\n",
    "                if save_path is not None:\n",
    "                    torch.save(best_state, save_path)  # Save to disk\n",
    "                    print(f\"Saved new best model to: {save_path}\")\n",
    "\n",
    "            del model  # Free memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "\n",
    "    # Rebuild best model\n",
    "    best_model = None\n",
    "    if best_state is not None:\n",
    "        best_model = model_fn(best_cfg['hidden_sizes']).to(device)  # Create model\n",
    "        best_model.load_state_dict(best_state)  # Load best weights\n",
    "\n",
    "\n",
    "    return results, best_cfg, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc50c5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
