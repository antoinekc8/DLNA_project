{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bc9df6",
   "metadata": {},
   "source": [
    "# Deep learning for dynamic network analysis (DLDNA) - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc303ce",
   "metadata": {},
   "source": [
    "Dolphins: R. ARNAUD M. DELPLANQUE A. KARILA-COHEN A. RAMPOLDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913557e",
   "metadata": {},
   "source": [
    "Comprehensive soil classification dataset: https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10532874",
   "metadata": {},
   "source": [
    "CNN puis GAN puis CyGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd56d4",
   "metadata": {},
   "source": [
    "Binary classification : Binary CrossEntropy Loss $ \\mathcal{L}_{\\text{BCE}}(y,\\hat y)\n",
    "= - \\left[ y \\log(\\hat y) + (1-y)\\log(1-\\hat y) \\right]\n",
    " $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73b84e",
   "metadata": {},
   "source": [
    "Rappel: le learning rate $ \\alpha $ est le pas de mise à jour lors de la descente de gradient. Formule de la descente de gradient $ L(\\theta_{n+1})= L(\\theta_{n})-\\alpha \\nabla  L(\\theta_{n}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9363973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded. Device: cpu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168df32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('..', '..\\\\data', '..\\\\data\\\\Orignal-Dataset')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '..'\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "images_dir = os.path.join(data_dir, 'Orignal-Dataset')\n",
    "root_dir, data_dir, images_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "221e14e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alluvial_Soil: 51 images loaded\n",
      "Arid_Soil: 284 images loaded\n",
      "Black_Soil: 255 images loaded\n",
      "Laterite_Soil: 219 images loaded\n",
      "Mountain_Soil: 201 images loaded\n",
      "Red_Soil: 109 images loaded\n",
      "Yellow_Soil: 69 images loaded\n"
     ]
    }
   ],
   "source": [
    "# Function to load all images from a folder\n",
    "\n",
    "def load_images_from_folder(folder_path, resize=None):\n",
    "    \"\"\"\n",
    "    Load all images from a folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path: path to the folder\n",
    "        resize: tuple (width, height) to resize images, None to skip resizing\n",
    "    \n",
    "    Returns:\n",
    "        list: list of tuples (image, filename)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder {folder_path} does not exist\")\n",
    "        return images\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Use PIL to load image and convert to numpy array (BGR format for OpenCV compatibility)\n",
    "                img = Image.open(filepath).convert('RGB')\n",
    "                img_array = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                if resize:\n",
    "                    img_array = cv2.resize(img_array, resize)\n",
    "                images.append((img_array, filename))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {e}\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load images from Orignal-Dataset\n",
    "soil_types = ['Alluvial_Soil', 'Arid_Soil', 'Black_Soil', 'Laterite_Soil', \n",
    "              'Mountain_Soil', 'Red_Soil', 'Yellow_Soil']\n",
    "\n",
    "images_dict = {}\n",
    "for soil_type in soil_types:\n",
    "    folder_path = os.path.join(data_dir, 'Orignal-Dataset', soil_type)\n",
    "    images_dict[soil_type] = load_images_from_folder(folder_path)\n",
    "    print(f\"{soil_type}: {len(images_dict[soil_type])} images loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7215ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "soil_types = [\n",
    "    'Alluvial_Soil', 'Arid_Soil', 'Black_Soil',\n",
    "    'Laterite_Soil', 'Mountain_Soil',\n",
    "    'Red_Soil', 'Yellow_Soil'\n",
    "]\n",
    "\n",
    "IMG_W, IMG_H = 80, 80\n",
    "PAD = 10\n",
    "LABEL_W = 180\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "rows = []\n",
    "\n",
    "for soil in soil_types:\n",
    "    imgs = images_dict.get(soil, [])\n",
    "    row_imgs = []\n",
    "\n",
    "    # zone label à gauche\n",
    "    label_img = np.zeros((IMG_H, LABEL_W, 3), dtype=np.uint8)\n",
    "    cv2.putText(\n",
    "        label_img,\n",
    "        soil,\n",
    "        (10, IMG_H // 2),\n",
    "        FONT,\n",
    "        0.5,\n",
    "        (255, 255, 255),\n",
    "        1,\n",
    "        cv2.LINE_AA\n",
    "    )\n",
    "\n",
    "    row_imgs.append(label_img)\n",
    "\n",
    "    for i in range(min(5, len(imgs))):\n",
    "        img, _ = imgs[i]\n",
    "        img_resized = cv2.resize(img, (IMG_W, IMG_H))\n",
    "        row_imgs.append(img_resized)\n",
    "\n",
    "    # padding images manquantes\n",
    "    while len(row_imgs) < 1 + 5:\n",
    "        row_imgs.append(np.zeros((IMG_H, IMG_W, 3), dtype=np.uint8))\n",
    "\n",
    "    # ajouter padding horizontal\n",
    "    padded_row = []\n",
    "    for im in row_imgs:\n",
    "        padded_row.append(im)\n",
    "        padded_row.append(np.zeros((IMG_H, PAD, 3), dtype=np.uint8))\n",
    "\n",
    "    rows.append(cv2.hconcat(padded_row[:-1]))  # enlever dernier padding\n",
    "\n",
    "# padding vertical entre lignes\n",
    "final_rows = []\n",
    "for r in rows:\n",
    "    final_rows.append(r)\n",
    "    final_rows.append(np.zeros((PAD, r.shape[1], 3), dtype=np.uint8))\n",
    "\n",
    "grid = cv2.vconcat(final_rows[:-1])\n",
    "\n",
    "cv2.imshow(\"Soil types – 5 images per category\", grid)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40d16d",
   "metadata": {},
   "source": [
    "Penser à convertir les X et y en tenseur torch avant de procéder au datasplit via ``sklearn.model_selection.train_test_split`` <br>\n",
    "```X = torch.FloatTensor(X) ``` et ```y = torch.FloatTensor(y) ``` <br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99004037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # First split: separate out test set (30% of total)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=SEED  # 70% train, 30% temp\n",
    "# )\n",
    "# # Second split: split temp into validation (10%) and test (20%)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=2/3, random_state=SEED  # 10% val, 20% test\n",
    "# )\n",
    "# print(f\"Training samples: {len(X_train)}\")  # Show split sizes\n",
    "# print(f\"Validation samples: {len(X_val)}\")\n",
    "# print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23bc75",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "Standardizer les data pour RGB et passer de 0 à 255 à 0 à 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc50c5",
   "metadata": {},
   "source": [
    "**My first CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61a76832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First convolutional layer: 3 input channels -> 32 output channels\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer: 32 -> 64 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Max pooling layer: reduces spatial dimensions by 2\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # First fully connected layer: 64*32*3 flattened features -> 128\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        # Output layer: 128 -> 10 classes\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block: apply conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Second conv block: apply conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten for fully connected layers: (batch, 64, 32, 32) -> (batch, 64*32*32)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # First FC layer with relu activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Output layer (no activation - CrossEntropyLoss handles softmax)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e514c3",
   "metadata": {},
   "source": [
    "**Definition of the MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "867d5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)  # Create model and move to device\n",
    "regression_loss_fn = nn.CrossEntropyLoss()  # Binary Cross Entropy loss\n",
    "regression_optimizer = Adam(model.parameters(), lr=LEARNING_RATE)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093ae72",
   "metadata": {},
   "source": [
    "**Grid search (hyperparameters optimisation for Adam GD): hidden size and learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b81d",
   "metadata": {},
   "source": [
    "If the learning rate is too big, the gradient descent cannot be stable. In a contrary if it is too small, the learning can be slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a22c",
   "metadata": {},
   "source": [
    "**Training with validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=100):\n",
    "    \"\"\"Train the model with validation monitoring.\"\"\"\n",
    "    train_losses = []  # Track training losses\n",
    "    val_losses = []  # Track validation losses\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()  # Set to training mode\n",
    "        train_loss = 0.0  # Reset epoch loss\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device),  yb.to(device)  # Move to device\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            preds = model(xb)  # Forward pass\n",
    "            loss = loss_fn(preds, yb)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            train_loss += loss.item()  # Accumulate loss\n",
    "        train_loss /= len(train_loader)  # Average loss\n",
    "        train_losses.append(train_loss)  # Store loss\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set to evaluation mode\n",
    "        val_loss = 0.0  # Reset validation loss\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device),  yb.to(device)  # Move to device\n",
    "                preds = model(xb)  # Forward pass\n",
    "                val_loss += loss_fn(preds, yb).item()  # Compute loss\n",
    "            val_loss /= len(val_loader)  # Average loss\n",
    "            val_losses.append(val_loss)  # Store loss\n",
    "            \n",
    "        if epoch % 10 == 0 or epoch == 1:  # Print every 10 epochs\n",
    "            print(f\"Epoch {epoch:03d} | Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9649c6f",
   "metadata": {},
   "source": [
    "**Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67953dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn, device):\n",
    "    \"\"\"Perform final testing on the model using the held-out test set.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    test_loss = 0.0  # Initialize test loss\n",
    "    all_preds = []  # Store all predictions\n",
    "    all_actuals = []  # Store all actual values\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)  # Move batch to device\n",
    "            yb = yb.to(device)  # Move labels to device\n",
    "            preds = model(xb)  # Forward pass\n",
    "            test_loss += loss_fn(preds, yb).item()  # Accumulate loss\n",
    "            all_preds.append(preds)  # Store predictions\n",
    "            all_actuals.append(yb)  # Store actuals\n",
    "            \n",
    "    test_loss /= len(test_loader)  # Average loss\n",
    "    all_preds = torch.cat(all_preds)  # Concatenate all predictions\n",
    "    all_actuals = torch.cat(all_actuals)  # Concatenate all actuals\n",
    "    \n",
    "    return test_loss, all_preds, all_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f1d59",
   "metadata": {},
   "source": [
    "**Test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 5e-4] \n",
    "hidden_sizes_options = [(32,16),(64,32), (128,64)]\n",
    "\n",
    "def grid_search_hyperparameters(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    learning_rates,\n",
    "    hidden_sizes_options,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    base_model=None,\n",
    "    model_fn=None,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid search over learning rates and hidden layer sizes.\n",
    "    - model_fn: callable taking hidden_sizes (e.g., (h1, h2)) and returning an nn.Module (on CPU).\n",
    "    - hidden_sizes_options: list of tuples like [(64,32), (128,64), ...]\n",
    "    Saves the globally best model (by validation accuracy) to 'save_path'.\n",
    "    Returns: (results_list, best_cfg_dict, best_model_loaded)\n",
    "    \"\"\"\n",
    "    assert model_fn is not None, \"Provide model_fn(hidden_sizes) -> nn.Module\"\n",
    "\n",
    "    results = []  # Store all results\n",
    "    best_val_acc = -1.0  # Track best validation accuracy\n",
    "    best_cfg = None  # Track best configuration\n",
    "    best_state = None  # Track best model state\n",
    "\n",
    "    for lr in learning_rates:  # Loop over learning rates\n",
    "        for hidden_sizes in hidden_sizes_options:  # Loop over architectures\n",
    "            print(f\"Testing: lr={lr}, hidden_sizes={hidden_sizes}\")\n",
    "\n",
    "            model = model_fn(hidden_sizes).to(device)  # Create model\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Create optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()  # Define loss function\n",
    "\n",
    "            # Train model\n",
    "            _, _, _, train_accuracies, val_accuracies = train_with_validation(\n",
    "                model=model,  # Model to train\n",
    "                train_loader=train_loader,  # Training data\n",
    "                val_loader=val_loader,  # Validation data\n",
    "                optimizer=optimizer,  # Optimizer\n",
    "                loss_fn=loss_fn,  # Loss function\n",
    "                device=device,  # Device\n",
    "                epochs=epochs,  # Number of epochs\n",
    "                task_type='classification'  # Task type\n",
    "            )\n",
    "\n",
    "            cur_best_val = max(val_accuracies)  # Get best validation accuracy\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'lr': lr,  # Learning rate\n",
    "                'hidden_sizes': hidden_sizes,  # Architecture\n",
    "                'best_val_acc': cur_best_val,  # Best validation accuracy\n",
    "                'final_train_acc': train_accuracies[-1],  # Final training accuracy\n",
    "                'final_val_acc': val_accuracies[-1]  # Final validation accuracy\n",
    "            })\n",
    "\n",
    "            print(f\"Best validation accuracy: {cur_best_val:.2f}%\")\n",
    "\n",
    "            # Update best model if this is better\n",
    "            if cur_best_val > best_val_acc:\n",
    "                best_val_acc = cur_best_val  # Update best accuracy\n",
    "                best_cfg = {'lr': lr, 'hidden_sizes': hidden_sizes}  # Update best config\n",
    "                best_state = {k: v.cpu() for k, v in model.state_dict().items()}  # Save state to CPU\n",
    "                if save_path is not None:\n",
    "                    torch.save(best_state, save_path)  # Save to disk\n",
    "                    print(f\"Saved new best model to: {save_path}\")\n",
    "\n",
    "            del model  # Free memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "\n",
    "    # Rebuild best model\n",
    "    best_model = None\n",
    "    if best_state is not None:\n",
    "        best_model = model_fn(best_cfg['hidden_sizes']).to(device)  # Create model\n",
    "        best_model.load_state_dict(best_state)  # Load best weights\n",
    "\n",
    "\n",
    "    return results, best_cfg, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469136c",
   "metadata": {},
   "source": [
    "**Train validate and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train the model with validation monitoring\n",
    "model, train_losses, val_losses = train_with_validation(\n",
    "    model=regression_model,  # Model to train\n",
    "    train_loader=train_loader,  # Training data\n",
    "    val_loader=val_loader,  # Validation data\n",
    "    optimizer=regression_optimizer,  # Optimizer\n",
    "    loss_fn=regression_loss_fn,  # Loss function\n",
    "    device=device,  # Device (CPU/GPU)\n",
    "    epochs=100  # Number of epochs\n",
    ")\n",
    "\n",
    "# Step 2: Perform final testing on held-out test set\n",
    "test_loss, test_preds, test_actuals = test_model(\n",
    "    model=model,  # Trained model\n",
    "    test_loader=test_loader,  # Test data\n",
    "    loss_fn=regression_loss_fn,  # Loss function\n",
    "    device=device  # Device\n",
    ")\n",
    "\n",
    "# Print evaluation metrics\n",
    "avg_train_loss = sum(train_losses) / len(train_losses)  # Calculate average training loss\n",
    "avg_val_loss = sum(val_losses) / len(val_losses)  # Calculate average validation loss\n",
    "print(f\"Average Training MSE: {avg_train_loss:.4f} | \"\n",
    "      f\"Average Validation MSE: {avg_val_loss:.4f} | \"\n",
    "      f\"Test MSE: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697ba96",
   "metadata": {},
   "source": [
    "**Training and validation visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd268f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and validation loss over epochs\n",
    "def plot_training_results(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))  # Create figure\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)  # Plot training\n",
    "    plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)  # Plot validation\n",
    "    plt.xlabel('Epoch')  # X-axis label\n",
    "    plt.ylabel('Loss')  # Y-axis label\n",
    "    plt.title('Training and Validation Loss Over Epochs')  # Title\n",
    "    plt.legend()  # Show legend\n",
    "    plt.grid(True, alpha=0.3)  # Add grid\n",
    "    plt.show()  # Display plot\n",
    "\n",
    "plot_training_results(train_losses, val_losses)  # Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e6a72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfe023f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff583df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\"\n",
    "    Standardize an image to have zero mean and unit variance per channel.\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array of shape (H, W, C)\n",
    "\n",
    "    Returns:\n",
    "        standardized_image: numpy array of same shape as input\n",
    "    \"\"\"\n",
    "    standardized_image = np.zeros_like(image, dtype=np.float32)\n",
    "    for c in range(image.shape[2]):\n",
    "        channel = image[:, :, c]\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        if std > 0:\n",
    "            standardized_image[:, :, c] = (channel - mean) / std\n",
    "        else:\n",
    "            standardized_image[:, :, c] = channel - mean  # Avoid division by zero\n",
    "    return standardized_image\n",
    "\n",
    "# Standardize all images in the dataset\n",
    "standardized_images_dict = {}\n",
    "for soil_type, images in images_dict.items():\n",
    "    standardized_images = []\n",
    "    for img, filename in images:\n",
    "        standardized_img = standardize_image(img)\n",
    "        standardized_images.append((standardized_img, filename))\n",
    "    standardized_images_dict[soil_type] = standardized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0707df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train set, test set and validation set\n",
    "\n",
    "def split_dataset(images_dict, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        images_dict: dictionary with soil types as keys and lists of (image, filename) tuples as values\n",
    "        train_ratio: proportion of data to use for training\n",
    "        val_ratio: proportion of data to use for validation\n",
    "        test_ratio: proportion of data to use for testing\n",
    "    Returns:\n",
    "        train_set, val_set, test_set: dictionaries with the same structure as images_dict\n",
    "    \"\"\"\n",
    "    train_set = {}\n",
    "    val_set = {}\n",
    "    test_set = {}\n",
    "    \n",
    "    for soil_type, images in images_dict.items():\n",
    "        np.random.shuffle(images)  # Shuffle images to ensure randomness\n",
    "        total_images = len(images)\n",
    "        train_end = int(total_images * train_ratio)\n",
    "        val_end = train_end + int(total_images * val_ratio)\n",
    "        \n",
    "        train_set[soil_type] = images[:train_end]\n",
    "        val_set[soil_type] = images[train_end:val_end]\n",
    "        test_set[soil_type] = images[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = split_dataset(standardized_images_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84111c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0e9a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 828\n",
      "Validation set size: 115\n",
      "Test set size: 245\n"
     ]
    }
   ],
   "source": [
    "# Create a custom Dataset class for PyTorch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, images_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dict: dictionary with soil types as keys and lists of (image, filename) tuples\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Create a mapping from soil type to index\n",
    "        self.soil_type_to_idx = {soil_type: idx for idx, soil_type in enumerate(images_dict.keys())}\n",
    "        \n",
    "        # Flatten the dictionary into lists\n",
    "        for soil_type, images in images_dict.items():\n",
    "            label = self.soil_type_to_idx[soil_type]\n",
    "            for img, filename in images:\n",
    "                self.data.append(img)\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert image from (H, W, C) to (C, H, W) for PyTorch\n",
    "        img = self.data[idx]\n",
    "        img = np.transpose(img, (2, 0, 1))  # Convert to (C, H, W)\n",
    "        img_tensor = torch.FloatTensor(img)\n",
    "        label = self.labels[idx]\n",
    "        return img_tensor, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SoilDataset(train_set)\n",
    "val_dataset = SoilDataset(val_set)\n",
    "test_dataset = SoilDataset(test_set)\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b32daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) TRAIN / EVAL LOOPS\n",
    "# =========================\n",
    "def batch_accuracy(logits, y):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for x, y in loader:\n",
    "            x = x.to(DEVICE, non_blocking=True)\n",
    "            y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = regression_loss_fn(logits, y)\n",
    "\n",
    "            if train:\n",
    "                regression_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                regression_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc  += batch_accuracy(logits, y)\n",
    "            n_batches  += 1\n",
    "\n",
    "    return total_loss / n_batches, total_acc / n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f30cb9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 9) ENTRAÎNEMENT + VALIDATION\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     va_loss, va_acc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 13\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_loss, total_acc, n_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(train):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     14\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\romai\\anaconda3\\envs\\dldna\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\romai\\anaconda3\\envs\\dldna\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\romai\\anaconda3\\envs\\dldna\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\romai\\anaconda3\\envs\\dldna\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9) ENTRAÎNEMENT + VALIDATION\n",
    "# =========================\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 10) TEST FINAL\n",
    "# =========================\n",
    "te_loss, te_acc = run_epoch(test_loader, train=False)\n",
    "print(f\"TEST | loss {te_loss:.4f} acc {te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 12) (OPTION) CHECK RAPIDE D'UN BATCH\n",
    "# =========================\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch x:\", x.shape, x.dtype, x.min().item(), x.max().item())  # (B,3,128,128), float, [0,1]\n",
    "print(\"Batch y:\", y.shape, y.dtype, y.min().item(), y.max().item())  # (B,), long, [0..6]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
