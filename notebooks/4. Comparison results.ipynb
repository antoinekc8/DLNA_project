{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a4c36a",
   "metadata": {},
   "source": [
    "# **Deep learning for dynamic network analysis (DLDNA) - Final project**\n",
    "**Dolphins:** R. ARNAUD M. DELPLANQUE A. KARILA-COHEN A. RAMPOLDI <br> **Dataset:** Comprehensive soil classification dataset: https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0f048",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7926dc",
   "metadata": {},
   "source": [
    "# Notebook 4 — Comparative Analysis of Deep Learning Architectures\n",
    "\n",
    "This notebook consolidates and compares the results obtained from all previous experiments, providing a global assessment of the proposed deep learning approaches for soil classification.\n",
    "\n",
    "> ### Compared Approaches\n",
    "> The following models and training strategies are evaluated:\n",
    "> - **CNN (baseline)**: Convolutional neural network trained on the original dataset.\n",
    "> - **CNN + cGAN**: CNN trained with data augmented using a Conditional GAN.\n",
    "> - **CNN + StarGAN**: CNN trained with multi-domain augmented data generated by StarGAN.\n",
    "> - **Transformer (baseline)**: Swin Transformer leveraging self-attention mechanisms for global feature extraction.\n",
    "> - **Transformer + GAN augmentation**: Transformer trained using GAN- or StarGAN-augmented datasets.\n",
    ">\n",
    "> ### Evaluation Criteria\n",
    "> Models are compared using the following metrics and analyses:\n",
    "> - Overall classification accuracy,\n",
    "> - Confusion matrices for class-wise performance,\n",
    "> - Generalization behavior on the test set,\n",
    "> - Sensitivity to class imbalance and minority classes.\n",
    "\n",
    "### Objective\n",
    "The objectives of this comparative study are to:\n",
    "- Identify the most effective architecture for soil image classification,\n",
    "- Assess the impact of generative data augmentation strategies,\n",
    "- Analyze trade-offs between model complexity, performance, and robustness.\n",
    "\n",
    "### Outcome\n",
    "The results presented in this notebook form the basis for the final discussion and conclusions of the report, highlighting the strengths and limitations of each approach in a geotechni\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c000b",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3025a17",
   "metadata": {},
   "source": [
    "# Model Comparison Results\n",
    "\n",
    "This notebook compares the performance of different models:\n",
    "- **CNN**: Simple Convolutional Neural Network\n",
    "- **Transformer**: Swin Transformer (Attention Mechanism)\n",
    "- **GAN**: CNN with Conditional GAN Data Augmentation\n",
    "- **StarGAN**: Results from StarGAN experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1da2d6",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff129e4",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e8e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e025f12",
   "metadata": {},
   "source": [
    "## 2. Load Results from All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ CNN: No results found at ..\\models\\CNN\n",
      "✗ Transformer: No results found at ..\\models\\Attention\n",
      "✗ GAN: No results found at ..\\models\\GAN\n",
      "✗ StarGAN: No results found at ..\\models\\StarGAN\n",
      "\n",
      "Total models with results: 0\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "project_root = Path('..')\n",
    "results_root = project_root / 'results'\n",
    "comparison_root = results_root / 'comparison'\n",
    "comparison_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define model folders\n",
    "model_folders = {\n",
    "    'CNN': results_root / 'CNN',\n",
    "    'Transformer': results_root / 'Attention',\n",
    "    'GAN': results_root / 'GAN',\n",
    "    'StarGAN': results_root / 'StarGAN'\n",
    "}\n",
    "\n",
    "# Check which models have results\n",
    "available_models = {}\n",
    "for model_name, folder_path in model_folders.items():\n",
    "    if folder_path.exists() and any(folder_path.iterdir()):\n",
    "        available_models[model_name] = folder_path\n",
    "        print(f\"✓ {model_name}: Results found at {folder_path}\")\n",
    "    else:\n",
    "        print(f\"✗ {model_name}: No results found at {folder_path}\")\n",
    "\n",
    "print(f\"\\nTotal models with results: {len(available_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2886a0",
   "metadata": {},
   "source": [
    "## 3. Parse Performance Metrics from Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d80ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded metrics for 0 models\n"
     ]
    }
   ],
   "source": [
    "def parse_evaluation_report(file_path):\n",
    "    \"\"\"Parse evaluation report to extract key metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Try to extract accuracy\n",
    "    accuracy_match = re.search(r'[Tt]otal [Aa]ccuracy[:\\s]+(\\d+\\.?\\d*)%?', content)\n",
    "    if accuracy_match:\n",
    "        metrics['accuracy'] = float(accuracy_match.group(1)) / 100 if float(accuracy_match.group(1)) > 1 else float(accuracy_match.group(1))\n",
    "    \n",
    "    # Try to extract precision\n",
    "    precision_match = re.search(r'[Mm]acro [Pp]recision[:\\s]+(\\d+\\.?\\d*)', content)\n",
    "    if precision_match:\n",
    "        metrics['precision'] = float(precision_match.group(1))\n",
    "    \n",
    "    # Try to extract recall\n",
    "    recall_match = re.search(r'[Mm]acro [Rr]ecall[:\\s]+(\\d+\\.?\\d*)', content)\n",
    "    if recall_match:\n",
    "        metrics['recall'] = float(recall_match.group(1))\n",
    "    \n",
    "    # Try to extract F1\n",
    "    f1_match = re.search(r'[Mm]acro [Ff]1[- ][Ss]core[:\\s]+(\\d+\\.?\\d*)', content)\n",
    "    if f1_match:\n",
    "        metrics['f1_score'] = float(f1_match.group(1))\n",
    "    \n",
    "    # Also try weighted average from classification report\n",
    "    weighted_match = re.search(r'weighted avg\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)', content)\n",
    "    if weighted_match and not metrics:\n",
    "        metrics['precision'] = float(weighted_match.group(1))\n",
    "        metrics['recall'] = float(weighted_match.group(2))\n",
    "        metrics['f1_score'] = float(weighted_match.group(3))\n",
    "    \n",
    "    return metrics if metrics else None\n",
    "\n",
    "# Collect metrics from all models\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, folder_path in available_models.items():\n",
    "    # Try different possible filenames\n",
    "    possible_files = [\n",
    "        folder_path / 'evaluation_report.txt',\n",
    "        folder_path / 'overall_metrics.txt',\n",
    "        folder_path / 'metrics.txt',\n",
    "        folder_path / 'results.txt'\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        metrics = parse_evaluation_report(file_path)\n",
    "        if metrics:\n",
    "            all_metrics[model_name] = metrics\n",
    "            print(f\"✓ {model_name}: Metrics loaded from {file_path.name}\")\n",
    "            break\n",
    "    \n",
    "    if model_name not in all_metrics:\n",
    "        print(f\"✗ {model_name}: Could not find or parse metrics file\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded metrics for {len(all_metrics)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5790b29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics could be loaded. Please check if evaluation reports exist.\n"
     ]
    }
   ],
   "source": [
    "# Display collected metrics\n",
    "if all_metrics:\n",
    "    metrics_df = pd.DataFrame(all_metrics).T\n",
    "    metrics_df = metrics_df.sort_values('accuracy', ascending=False) if 'accuracy' in metrics_df.columns else metrics_df\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(metrics_df.to_string())\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No metrics could be loaded. Please check if evaluation reports exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96494e2c",
   "metadata": {},
   "source": [
    "## 4. Visualize Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813fb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data to create comparison plots\n"
     ]
    }
   ],
   "source": [
    "if all_metrics and len(all_metrics) > 0:\n",
    "    # Create comparison bar plots\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    available_metrics = [m for m in metrics_to_plot if m in metrics_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "        \n",
    "        for idx, metric in enumerate(available_metrics):\n",
    "            if idx < len(axes):\n",
    "                ax = axes[idx]\n",
    "                data = metrics_df[metric].sort_values(ascending=False)\n",
    "                bars = ax.bar(range(len(data)), data.values, color=colors[:len(data)])\n",
    "                ax.set_xticks(range(len(data)))\n",
    "                ax.set_xticklabels(data.index, rotation=45, ha='right')\n",
    "                ax.set_ylabel('Score')\n",
    "                ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
    "                ax.set_ylim([0, 1.0])\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar in bars:\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{height:.3f}',\n",
    "                           ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Remove extra subplots if any\n",
    "        for idx in range(len(available_metrics), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "        \n",
    "        plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.00)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        output_path = comparison_root / 'model_comparison_metrics.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nComparison plot saved to {output_path}\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to create comparison plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b5fe8",
   "metadata": {},
   "source": [
    "## 5. Radar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b68990",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_metrics and len(all_metrics) > 1:\n",
    "    # Create radar chart\n",
    "    from math import pi\n",
    "    \n",
    "    available_metrics = [m for m in ['accuracy', 'precision', 'recall', 'f1_score'] if m in metrics_df.columns]\n",
    "    \n",
    "    if len(available_metrics) >= 3:\n",
    "        # Number of variables\n",
    "        categories = [m.replace('_', ' ').title() for m in available_metrics]\n",
    "        N = len(categories)\n",
    "        \n",
    "        # Create angles for each metric\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # Initialize plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        # Plot each model\n",
    "        for idx, (model_name, row) in enumerate(metrics_df.iterrows()):\n",
    "            values = [row[m] for m in available_metrics]\n",
    "            values += values[:1]\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[idx % len(colors)])\n",
    "            ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n",
    "        \n",
    "        # Fix axis to go from 0 to 1\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories, size=12)\n",
    "        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "        plt.title('Model Performance Radar Chart', size=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Save figure\n",
    "        output_path = comparison_root / 'model_comparison_radar.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Radar chart saved to {output_path}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea5b3d",
   "metadata": {},
   "source": [
    "## 6. Load and Compare Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a44f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No confusion matrices found\n"
     ]
    }
   ],
   "source": [
    "# Display confusion matrices side by side\n",
    "confusion_matrices = {}\n",
    "\n",
    "for model_name, folder_path in available_models.items():\n",
    "    cm_path = folder_path / 'confusion_matrix.png'\n",
    "    if cm_path.exists():\n",
    "        confusion_matrices[model_name] = cm_path\n",
    "\n",
    "if confusion_matrices:\n",
    "    from PIL import Image\n",
    "    \n",
    "    n_models = len(confusion_matrices)\n",
    "    cols = min(2, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12*cols, 10*rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "    \n",
    "    for idx, (model_name, cm_path) in enumerate(confusion_matrices.items()):\n",
    "        img = Image.open(cm_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'{model_name} Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Remove extra subplots\n",
    "    for idx in range(len(confusion_matrices), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined figure\n",
    "    output_path = comparison_root / 'all_confusion_matrices.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nCombined confusion matrices saved to {output_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No confusion matrices found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766d6e0",
   "metadata": {},
   "source": [
    "## 7. Training Curves Comparison (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d302ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training curves found\n"
     ]
    }
   ],
   "source": [
    "# Display training curves side by side\n",
    "training_curves = {}\n",
    "\n",
    "for model_name, folder_path in available_models.items():\n",
    "    curve_path = folder_path / 'training_curves.png'\n",
    "    if curve_path.exists():\n",
    "        training_curves[model_name] = curve_path\n",
    "\n",
    "if training_curves:\n",
    "    from PIL import Image\n",
    "    \n",
    "    n_models = len(training_curves)\n",
    "    cols = min(2, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(14*cols, 6*rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "    \n",
    "    for idx, (model_name, curve_path) in enumerate(training_curves.items()):\n",
    "        img = Image.open(curve_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'{model_name} Training Curves', fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Remove extra subplots\n",
    "    for idx in range(len(training_curves), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined figure\n",
    "    output_path = comparison_root / 'all_training_curves.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Combined training curves saved to {output_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training curves found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7830a",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7011deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics available to generate summary report\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary report\n",
    "if all_metrics:\n",
    "    report_path = comparison_root / 'comparison_summary.txt'\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"COMPREHENSIVE MODEL COMPARISON REPORT\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Models Evaluated:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for model_name in all_metrics.keys():\n",
    "            f.write(f\"  - {model_name}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"Performance Metrics:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(metrics_df.to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Best performing model per metric\n",
    "        f.write(\"Best Performing Models:\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for metric in metrics_df.columns:\n",
    "            best_model = metrics_df[metric].idxmax()\n",
    "            best_score = metrics_df[metric].max()\n",
    "            f.write(f\"  {metric.replace('_', ' ').title():20s}: {best_model:15s} ({best_score:.4f})\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        if 'accuracy' in metrics_df.columns:\n",
    "            best_overall = metrics_df['accuracy'].idxmax()\n",
    "            f.write(f\"\\nRECOMMENDATION: {best_overall} shows the best overall accuracy.\\n\")\n",
    "        \n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nComprehensive summary report saved to {report_path}\")\n",
    "    \n",
    "    # Display the report\n",
    "    with open(report_path, 'r') as f:\n",
    "        print(\"\\n\" + f.read())\n",
    "else:\n",
    "    print(\"No metrics available to generate summary report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941d348",
   "metadata": {},
   "source": [
    "## 9. Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edef0e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics to export\n"
     ]
    }
   ],
   "source": [
    "# Save metrics to CSV for further analysis\n",
    "if all_metrics:\n",
    "    csv_path = comparison_root / 'model_comparison.csv'\n",
    "    metrics_df.to_csv(csv_path)\n",
    "    print(f\"\\nMetrics exported to {csv_path}\")\n",
    "    print(\"\\nYou can now use this CSV file for further analysis or reporting.\")\n",
    "else:\n",
    "    print(\"No metrics to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
