{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320fb513",
   "metadata": {},
   "source": [
    "# **Maxime's Notebook - Focus on Cycle GAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb7fe5",
   "metadata": {},
   "source": [
    "what is a cycle gan ?  \n",
    "CycleGAN augmentation → Image dataset → Classification model (CNN, ResNet, etc.) → Soil type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "463c3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tensorboard\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a1fc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data directories\n",
    "original_dataset_dir = r\"../data/Orignal-Dataset\"\n",
    "cyaug_dataset_dir = r\"../data/CyAUG-Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af627dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two versions of your dataset:\n",
    "# Domain A: Original images (from your data folder) -> pictures inside original_dataset_dir\n",
    "# Domain B: Same images with random augmentations applied\n",
    "\n",
    "class SoilDataset(Dataset):\n",
    "    \"\"\"Dataset loader for soil images organized by soil type subdirectories\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.soil_types = []\n",
    "        \n",
    "        # Get all soil type subdirectories\n",
    "        soil_type_dirs = [d for d in os.listdir(root_dir) \n",
    "                         if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        \n",
    "        # Collect all image paths\n",
    "        for soil_type in sorted(soil_type_dirs):\n",
    "            soil_dir = os.path.join(root_dir, soil_type)\n",
    "            for img_file in os.listdir(soil_dir):\n",
    "                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(soil_dir, img_file))\n",
    "                    self.soil_types.append(soil_type)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        soil_type = self.soil_types[idx]\n",
    "        \n",
    "        # Read image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, soil_type\n",
    "\n",
    "# Define transformations for Domain B (augmented version)\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256, 256)),\n",
    "    torchvision.transforms.RandomRotation(15),\n",
    "    torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.GaussianBlur(kernel_size=3),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms_original = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256, 256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b9faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Domain A (Original): 1188 images\n",
      "Domain B (Augmented): 1188 images\n",
      "\n",
      "Sample batch shape (Domain A): torch.Size([7, 3, 256, 256])\n",
      "Soil types in batch: ('Mountain_Soil', 'Black_Soil', 'Arid_Soil', 'Laterite_Soil', 'Mountain_Soil', 'Arid_Soil', 'Black_Soil')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets with proper structure handling\n",
    "print(\"Loading datasets...\")\n",
    "domain_A = SoilDataset(original_dataset_dir, transform=transforms_original)\n",
    "domain_B = SoilDataset(original_dataset_dir, transform=transforms)\n",
    "\n",
    "print(f\"Domain A (Original): {len(domain_A)} images\")\n",
    "print(f\"Domain B (Augmented): {len(domain_B)} images\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 7\n",
    "dataloader_A = DataLoader(domain_A, batch_size=batch_size, shuffle=True)\n",
    "dataloader_B = DataLoader(domain_B, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check if data loads correctly\n",
    "sample_img_A, soil_type = next(iter(dataloader_A))\n",
    "print(f\"\\nSample batch shape (Domain A): {sample_img_A.shape}\")\n",
    "print(f\"Soil types in batch: {soil_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2d7312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator and Discriminator classes defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import cycle\n",
    "\n",
    "# Define Generator (ResNet-based)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, 3),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, 3),\n",
    "            nn.InstanceNorm2d(in_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(256)]\n",
    "        \n",
    "        # Upsampling\n",
    "        model += [\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Final convolution layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, out_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define Discriminator (PatchGAN)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(in_channels, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, 4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"Generator and Discriminator classes defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e038bd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Models and optimizers initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "generator_A2B = Generator().to(device)\n",
    "generator_B2A = Generator().to(device)\n",
    "discriminator_A = Discriminator().to(device)\n",
    "discriminator_B = Discriminator().to(device)\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    list(generator_A2B.parameters()) + list(generator_B2A.parameters()),\n",
    "    lr=lr, betas=(beta1, 0.999)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(discriminator_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(discriminator_B.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "print(\"Models and optimizers initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CycleGAN training...\n",
      "Epochs: 10, Batch size: 7\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "lambda_cycle = 10.0  # Weight for cycle consistency loss\n",
    "lambda_identity = 5.0  # Weight for identity loss\n",
    "\n",
    "print(\"Starting CycleGAN training...\")\n",
    "print(f\"Epochs: {num_epochs}, Batch size: {batch_size}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_G = 0\n",
    "    epoch_loss_D = 0\n",
    "    \n",
    "    # Use itertools.cycle to handle different dataset sizes\n",
    "    for (real_A, _), (real_B, _) in zip(dataloader_A, cycle(dataloader_B)):\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "        \n",
    "        # --- Generator forward pass ---\n",
    "        # Generate fake images\n",
    "        fake_B = generator_A2B(real_A)\n",
    "        fake_A = generator_B2A(real_B)\n",
    "        \n",
    "        # Cycle consistency\n",
    "        cycle_A = generator_B2A(fake_B)\n",
    "        cycle_B = generator_A2B(fake_A)\n",
    "        \n",
    "        # Identity mapping\n",
    "        identity_A = generator_B2A(real_A)\n",
    "        identity_B = generator_A2B(real_B)\n",
    "        \n",
    "        # --- Discriminator losses ---\n",
    "        optimizer_D_A.zero_grad()\n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "        # Real/Fake discrimination\n",
    "        real_A_pred = discriminator_A(real_A)\n",
    "        fake_A_pred = discriminator_A(fake_A.detach())\n",
    "        real_B_pred = discriminator_B(real_B)\n",
    "        fake_B_pred = discriminator_B(fake_B.detach())\n",
    "        \n",
    "        # Loss computation\n",
    "        loss_D_A = criterion_GAN(real_A_pred, torch.ones_like(real_A_pred)) + \\\n",
    "                   criterion_GAN(fake_A_pred, torch.zeros_like(fake_A_pred))\n",
    "        loss_D_B = criterion_GAN(real_B_pred, torch.ones_like(real_B_pred)) + \\\n",
    "                   criterion_GAN(fake_B_pred, torch.zeros_like(fake_B_pred))\n",
    "        \n",
    "        loss_D = loss_D_A + loss_D_B\n",
    "        loss_D.backward()\n",
    "        optimizer_D_A.step()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # --- Generator losses ---\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # GAN loss\n",
    "        fake_A_pred = discriminator_A(fake_A)\n",
    "        fake_B_pred = discriminator_B(fake_B)\n",
    "        loss_GAN_A2B = criterion_GAN(fake_B_pred, torch.ones_like(fake_B_pred))\n",
    "        loss_GAN_B2A = criterion_GAN(fake_A_pred, torch.ones_like(fake_A_pred))\n",
    "        \n",
    "        # Cycle loss\n",
    "        loss_cycle_A = criterion_cycle(cycle_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(cycle_B, real_B)\n",
    "        loss_cycle = loss_cycle_A + loss_cycle_B\n",
    "        \n",
    "        # Identity loss\n",
    "        loss_identity_A = criterion_identity(identity_A, real_A)\n",
    "        loss_identity_B = criterion_identity(identity_B, real_B)\n",
    "        loss_identity = loss_identity_A + loss_identity_B\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + lambda_cycle * loss_cycle + lambda_identity * loss_identity\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        epoch_loss_G += loss_G.item()\n",
    "        epoch_loss_D += loss_D.item()\n",
    "    \n",
    "    avg_loss_G = epoch_loss_G / len(dataloader_A)\n",
    "    avg_loss_D = epoch_loss_D / len(dataloader_A)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - G Loss: {avg_loss_G:.4f}, D Loss: {avg_loss_D:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f45e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented images using trained generator\n",
    "output_dir = os.path.join(original_dataset_dir, \"..\", \"CycleGAN_Augmented\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Generating augmented images to: {output_dir}\")\n",
    "\n",
    "generator_A2B.eval()\n",
    "generator_B2A.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for soil_type in os.listdir(original_dataset_dir):\n",
    "        soil_type_path = os.path.join(original_dataset_dir, soil_type)\n",
    "        \n",
    "        if not os.path.isdir(soil_type_path):\n",
    "            continue\n",
    "        \n",
    "        # Create output directory for this soil type\n",
    "        output_soil_dir = os.path.join(output_dir, soil_type)\n",
    "        os.makedirs(output_soil_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each image\n",
    "        for img_file in os.listdir(soil_type_path):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "            \n",
    "            img_path = os.path.join(soil_type_path, img_file)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_tensor = transforms_original(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate augmented version\n",
    "            augmented = generator_A2B(image_tensor)\n",
    "            \n",
    "            # Convert back to image\n",
    "            augmented_img = augmented.squeeze(0).cpu()\n",
    "            augmented_img = (augmented_img * 0.5 + 0.5).clamp(0, 1)  # Denormalize\n",
    "            augmented_pil = torchvision.transforms.ToPILImage()(augmented_img)\n",
    "            \n",
    "            # Save augmented image\n",
    "            output_path = os.path.join(output_soil_dir, f\"augmented_{img_file}\")\n",
    "            augmented_pil.save(output_path)\n",
    "        \n",
    "        print(f\"✓ Processed {soil_type}\")\n",
    "\n",
    "print(f\"\\n✓ All augmented images saved to: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
