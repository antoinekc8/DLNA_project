{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b2ea3c",
   "metadata": {},
   "source": [
    "# **Deep learning for dynamic network analysis (DLDNA)** <br> Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709de10",
   "metadata": {},
   "source": [
    "**Dolphins:** R. ARNAUD M. DELPLANQUE A. KARILA-COHEN A. RAMPOLDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ec909",
   "metadata": {},
   "source": [
    "Comprehensive soil classification dataset: https://www.kaggle.com/datasets/ai4a-lab/comprehensive-soil-classification-datasets/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c8307",
   "metadata": {},
   "source": [
    "### **1. Preliminnary tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09158bd",
   "metadata": {},
   "source": [
    "**Import of the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00968f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings to keep notebook clean\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85253879",
   "metadata": {},
   "source": [
    "**Path configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e62f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent.resolve()\n",
    "DATA_DIR= PROJECT_ROOT / \"data\"\n",
    "PARAM_FILE = PROJECT_ROOT / \"txt\" / \"parameters.txt\"\n",
    "# utils.py functions\n",
    "UTILS_DIR = PROJECT_ROOT / \"src\"\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "from utils import load_parameters, load_images\n",
    "from visualization import show_soil_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580dffe",
   "metadata": {},
   "source": [
    "**Choose the good torch device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08c8b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded. Device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'xpu' if hasattr(torch, \"xpu\") and torch.xpu.is_available() else 'cpu'\n",
    "print(f\"Params loaded. Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887af6c8",
   "metadata": {},
   "source": [
    "**General parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9ac6785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded parameters:\n",
      "  TRAIN_RATIO = 0.7\n",
      "  VAL_RATIO = 0.1\n",
      "  TEST_RATIO = 0.2\n",
      "  BATCH_SIZE = 64\n",
      "  EPOCHS = 100\n",
      "  LEARNING_RATE = 0.01\n",
      "  SEED = 42\n",
      "  SOIL_TYPES = Alluvial_Soil,Arid_Soil,Black_Soil,Laterite_Soil,Mountain_Soil,Red_Soil,Yellow_Soil\n"
     ]
    }
   ],
   "source": [
    "# Load parameters from external file\n",
    "params = load_parameters(PARAM_FILE)\n",
    "globals().update(params)\n",
    "soil_types = params[\"SOIL_TYPES\"].split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdb1c2",
   "metadata": {},
   "source": [
    "**Seeding to ensure reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be6729b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1edf708f890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use parameters for seed and device\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09ac6f",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "365a566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dict = {}\n",
    "\n",
    "# for soil in soil_types:\n",
    "#     folder = DATA_DIR / \"Orignal-Dataset\" / soil\n",
    "#     images_dict[soil] = load_images(folder)\n",
    "#     print(f\"{soil}: {len(images_dict[soil])} images loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e06a9",
   "metadata": {},
   "source": [
    "**Display the first pictures of each type of soil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "177db2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dict = {soil_type: [(img_bgr, filename), ...], ...}\n",
    "show_soil_grid(images_dict, n_per_type=5, tile_size=(240, 240), pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84add3",
   "metadata": {},
   "source": [
    "**Create Results folder to store the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf07a1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified results root: C:\\Users\\romai\\OneDrive\\Documents\\_ENTPE\\Formation\\Data science\\DLDNA\\DLDNA - Project\\results\n",
      " - C:\\Users\\romai\\OneDrive\\Documents\\_ENTPE\\Formation\\Data science\\DLDNA\\DLDNA - Project\\results\\CNN\n",
      " - C:\\Users\\romai\\OneDrive\\Documents\\_ENTPE\\Formation\\Data science\\DLDNA\\DLDNA - Project\\results\\GAN\n",
      " - C:\\Users\\romai\\OneDrive\\Documents\\_ENTPE\\Formation\\Data science\\DLDNA\\DLDNA - Project\\results\\StarGAN\n",
      " - C:\\Users\\romai\\OneDrive\\Documents\\_ENTPE\\Formation\\Data science\\DLDNA\\DLDNA - Project\\results\\Attention\n"
     ]
    }
   ],
   "source": [
    "# Create results folder structure\n",
    "from pathlib import Path\n",
    "\n",
    "results_root = (Path('..') / 'results').resolve()\n",
    "subfolders = ['CNN', 'GAN', 'StarGAN', 'Attention']\n",
    "\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "for name in subfolders:\n",
    "    (results_root / name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created/verified results root: {results_root}\")\n",
    "for name in subfolders:\n",
    "    print(f\" - {results_root / name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fae94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First convolutional layer: 3 input channels -> 32 output channels\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Second convolutional layer: 32 -> 64 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Third convolutional layer: 64 -> 128 channels\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling layer: reduces spatial dimensions by 2 each time\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Adaptive pooling: makes the feature map always 1x1 (independent of input size)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layer: 128 pooled features -> 128\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "\n",
    "        # Dropout to reduce overfitting\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        # Output layer: 128 -> 7 classes\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block: conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Second conv block: conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Third conv block: conv -> relu -> pool\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Global average pooling: (batch, 128, H, W) -> (batch, 128, 1, 1)\n",
    "        x = self.gap(x)\n",
    "\n",
    "        # Flatten: (batch, 128, 1, 1) -> (batch, 128)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # FC + relu + dropout\n",
    "        x = self.drop(F.relu(self.fc1(x)))\n",
    "\n",
    "        # Output (CrossEntropyLoss applies softmax internally)\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3dbb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_image(image):\n",
    "    standardized_image = np.zeros_like(image, dtype=np.float32)\n",
    "    for c in range(image.shape[2]):\n",
    "        channel = image[:, :, c]\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        if std > 0:\n",
    "            standardized_image[:, :, c] = (channel - mean) / std\n",
    "        else:\n",
    "            standardized_image[:, :, c] = channel - mean\n",
    "    return standardized_image\n",
    "\n",
    "standardized_images_dict = {}\n",
    "for soil_type, images in images_dict.items():\n",
    "    standardized_images = []\n",
    "    for img, filename in images:\n",
    "        standardized_img = standardize_image(img)\n",
    "        standardized_images.append((standardized_img, filename))\n",
    "    standardized_images_dict[soil_type] = standardized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03d90bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: train (831), val (119), test (238)\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "for soil_type, images in standardized_images_dict.items():\n",
    "    for img, filename in images:\n",
    "        all_images.append(img)\n",
    "        all_labels.append(soil_type)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(all_images, all_labels, test_size=TEST_RATIO, random_state=SEED, stratify=all_labels)\n",
    "relative_val_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=relative_val_ratio, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Dataset split: train ({len(X_train)}), val ({len(X_val)}), test ({len(X_test)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be02e8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created - Train: 831, Val: 119, Test: 238\n"
     ]
    }
   ],
   "source": [
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.data = images\n",
    "        self.labels = labels\n",
    "        unique_labels = sorted(list(set(labels)))\n",
    "        self.soil_type_to_idx = {soil_type: idx for idx, soil_type in enumerate(unique_labels)}\n",
    "        self.label_indices = [self.soil_type_to_idx[label] for label in labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):         \n",
    "        img = self.data[idx]\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img_tensor = torch.FloatTensor(img)\n",
    "        label = self.label_indices[idx]\n",
    "        return img_tensor, label\n",
    "\n",
    "train_dataset = SoilDataset(X_train, y_train)\n",
    "val_dataset = SoilDataset(X_val, y_val)\n",
    "test_dataset = SoilDataset(X_test, y_test)\n",
    "print(f\"Datasets created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10bb5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(DEVICE)  # Create model and move to device\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # Binary Cross Entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # Adam optimizer using weight decay which helps regularization in order to reduce overfitting\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fccc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24fab5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SimpleCNN Architecture Inspection ===\n",
      "==================================================\n",
      " 2. conv1                | Conv2d          | Params:      896\n",
      " 3. conv2                | Conv2d          | Params:    18496\n",
      " 4. conv3                | Conv2d          | Params:    73856\n",
      " 5. pool                 | MaxPool2d       | Params:        0\n",
      " 6. gap                  | AdaptiveAvgPool2d | Params:        0\n",
      " 7. fc1                  | Linear          | Params:    16512\n",
      " 8. drop                 | Dropout         | Params:        0\n",
      " 9. fc2                  | Linear          | Params:      903\n",
      "==================================================\n",
      "Total Parameters: 110,663\n",
      "Trainable Parameters: 110,663\n",
      "Non-trainable Parameters: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110663"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with our blueprint model\n",
    "from util_2 import inspect_architecture\n",
    "inspect_architecture(model, \"SimpleCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 1.9708 Acc: 0.218 | Val Loss: 1.7903 Acc: 0.235\n",
      "Epoch 010 | Train Loss: 1.6421 Acc: 0.309 | Val Loss: 1.6744 Acc: 0.303\n",
      "Epoch 020 | Train Loss: 1.5383 Acc: 0.367 | Val Loss: 1.7474 Acc: 0.353\n"
     ]
    }
   ],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=100):\n",
    "    \"\"\"Train the model with validation monitoring.\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            preds = model(xb) # Forward pass\n",
    "            loss = loss_fn(preds, yb) # Compute loss\n",
    "            loss.backward() # Backward pass calcule et stocke les gradients sur chaque paramÃ¨tre\n",
    "            optimizer.step() # Update weights using the gradients\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += loss_fn(preds, yb).item()\n",
    "                correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += loss_fn(preds, yb).item()\n",
    "                correct += (preds.argmax(dim=1) == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:03d} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f}\"\n",
    "            )\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "model, train_losses, val_losses, train_accs, val_accs = train_with_validation(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00029697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Perform final testing on the model using the held-out test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(xb)\n",
    "\n",
    "            # Accumulate loss\n",
    "            test_loss += loss_fn(logits, yb).item()\n",
    "\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(yb)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return test_loss, all_logits, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loss, logits, labels = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "labels = labels.cpu().numpy()\n",
    "\n",
    "accuracy  = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "recall    = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "f1        = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET RESULTS - CNN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Enhanced Confusion Matrix Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. Raw Confusion Matrix (Counts)\n",
    "ax = axes[0, 0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=soil_types, yticklabels=soil_types,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Confusion Matrix - Raw Counts', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# 2. Normalized Confusion Matrix (Percentages by True Class) - safe division\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "cm_normalized = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums != 0) * 100\n",
    "\n",
    "ax = axes[0, 1]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='RdYlGn', ax=ax,\n",
    "            xticklabels=soil_types, yticklabels=soil_types,\n",
    "            cbar_kws={'label': 'Percentage (%)'}, vmin=0, vmax=100)\n",
    "ax.set_title('Confusion Matrix - Normalized (% per True Class)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# 3. Correct Predictions Heatmap (Diagonal Only)\n",
    "cm_correct = np.zeros_like(cm, dtype=float)\n",
    "for i in range(len(soil_types)):\n",
    "    cm_correct[i, i] = cm[i, i]\n",
    "\n",
    "ax = axes[1, 0]\n",
    "sns.heatmap(cm_correct, annot=True, fmt='d', cmap='Greens', ax=ax,\n",
    "            xticklabels=soil_types, yticklabels=soil_types,\n",
    "            cbar_kws={'label': 'Correct Predictions'})\n",
    "ax.set_title('Correct Predictions per Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=11)\n",
    "ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# 4. Per-Class Accuracy Bar Chart - safe division\n",
    "class_totals = cm.sum(axis=1)\n",
    "class_accuracies = np.divide(np.diag(cm), class_totals, out=np.zeros_like(class_totals, dtype=float), where=class_totals != 0) * 100\n",
    "\n",
    "colors = ['#2ecc71' if acc >= 70 else '#f39c12' if acc >= 50 else '#e74c3c' for acc in class_accuracies]\n",
    "ax = axes[1, 1]\n",
    "bars = ax.bar(soil_types, class_accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "mean_acc = np.mean(class_accuracies) if len(class_accuracies) else 0.0\n",
    "ax.axhline(y=mean_acc, color='blue', linestyle='--', linewidth=2, label=f'Mean Accuracy: {mean_acc:.1f}%')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Soil Type', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Per-Class Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, 105])\n",
    "ax.set_xticklabels(soil_types, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, class_accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed accuracy summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED ACCURACY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "for i, soil_type in enumerate(soil_types):\n",
    "    correct = int(cm[i, i])\n",
    "    total = int(cm[i].sum())\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0.0\n",
    "    print(f\"{soil_type:20s}: {correct:3d}/{total:3d} = {accuracy:6.2f}%\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222be10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to GAN folder\n",
    "print(\"\\nSaving results to GAN folder...\\n\")\n",
    "\n",
    "results_dir = Path('..') / 'results' / 'CNN'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suggested search space for CNN training\n",
    "learning_rates = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
    "dropout_options = [0.0, 0.2, 0.3, 0.5]\n",
    "weight_decay_options = [0.0, 1e-5, 1e-4]\n",
    "\n",
    "def grid_search_hyperparameters(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    learning_rates,\n",
    "    dropout_options,\n",
    "    weight_decay_options,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    model_fn=None,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid search over learning rate, dropout and weight decay.\n",
    "    - model_fn: callable taking (dropout) and returning an nn.Module (on CPU).\n",
    "    Saves the globally best model (by validation accuracy) to 'save_path'.\n",
    "    Returns: (results_list, best_cfg_dict, best_model_loaded)\n",
    "    \"\"\"\n",
    "    assert model_fn is not None, \"Provide model_fn(dropout) -> nn.Module\"\n",
    "\n",
    "    results = []           # Store all results\n",
    "    best_val_acc = -1.0    # Track best validation accuracy\n",
    "    best_cfg = None        # Track best configuration\n",
    "    best_state = None      # Track best model state\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for dropout in dropout_options:\n",
    "            for wd in weight_decay_options:\n",
    "                print(f\"Testing: lr={lr}, dropout={dropout}, weight_decay={wd}\")\n",
    "\n",
    "                # Create model\n",
    "                model = model_fn(dropout).to(device)\n",
    "\n",
    "                # Optimizer + weight decay\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    model.parameters(),\n",
    "                    lr=lr,\n",
    "                    weight_decay=wd\n",
    "                )\n",
    "\n",
    "                # Loss\n",
    "                loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "                # Train model\n",
    "                model, train_losses, val_losses, train_accs, val_accs = train_with_validation(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    device=device,\n",
    "                    epochs=epochs\n",
    "                )\n",
    "\n",
    "                cur_best_val = max(val_accs)  # Best validation accuracy over epochs\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'lr': lr,\n",
    "                    'dropout': dropout,\n",
    "                    'weight_decay': wd,\n",
    "                    'best_val_acc': cur_best_val,\n",
    "                    'final_train_acc': train_accs[-1],\n",
    "                    'final_val_acc': val_accs[-1]\n",
    "                })\n",
    "\n",
    "                print(f\"Best validation accuracy: {cur_best_val:.4f}\")\n",
    "\n",
    "                # Update best model\n",
    "                if cur_best_val > best_val_acc:\n",
    "                    best_val_acc = cur_best_val\n",
    "                    best_cfg = {'lr': lr, 'dropout': dropout, 'weight_decay': wd}\n",
    "                    best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "                    if save_path is not None:\n",
    "                        torch.save(best_state, save_path)\n",
    "                        print(f\"Saved new best model to: {save_path}\")\n",
    "\n",
    "                del model\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "    # Rebuild best model\n",
    "    best_model = None\n",
    "    if best_state is not None:\n",
    "        best_model = model_fn(best_cfg['dropout']).to(device)\n",
    "        best_model.load_state_dict(best_state)\n",
    "\n",
    "    return results, best_cfg, best_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
